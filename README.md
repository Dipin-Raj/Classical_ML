# ğŸ§  Predictive Modeling using Regression and Classification

---

## ğŸ“‚ Repository Structure
```
â”‚â”€â”€ Notebook/ # Contains Google Colab notebooks for each task
â”‚ â”œâ”€â”€ OMNIe_Task_1.ipynb
â”‚ â”œâ”€â”€ OMNIe_Task_2.ipynb
â”‚
â”‚â”€â”€ Graphs/ # All EDA visualizations, ROC curves, correlation plots
â”‚
â”‚â”€â”€ Report & Documentation/ # Project reports and supporting documents
â”‚ â”œâ”€â”€ OMNIe Solutions_Predicting Operational Efficiency of Manufacturing Teams.pdf
â”‚ â”œâ”€â”€ OMNIe Solutions_Retail Web Session Intelligence.pdf
â”‚
â”‚â”€â”€ README.md # Project overview (this file)
```

## ğŸ§© TASK 1: Predicting Team Efficiency in a Manufacturing Unit

### ğŸ“˜ Dataset
This dataset records the **daily operational statistics** of production teams in a garment manufacturing plant.  
Each record represents a single dayâ€™s performance of a team, including:
- **Production Features:** `plannedEfficiency`, `standardMinuteValue`, `workInProgress`, `performanceBonus`
- **Workforce Features:** `idleMinutes`, `workerCount`, `idleWorkers`
- **Contextual Features:** `dayOfWeek`, `productionDept`, etc.  
- **Target Variable:** `efficiencyScore`

### ğŸ¯ Objective
To build a **regression model** that predicts team efficiency based on production and workforce-related variables, identifying the most influential factors behind daily operational performance.

### âš™ï¸ Methodology
- Data Cleaning & Missing Value Treatment  
- Outlier Mitigation using **range-based thresholds from scatter plots**  
- Department-wise Segmentation (Stitching vs Finishing & Quality)  
- Feature Scaling and One-Hot Encoding  
- Model Training using:
  - Linear Regression (Scratch + Sklearn)
  - Decision Tree Regressor
  - Random Forest Regressor

### ğŸ“Š Results
| Model | RÂ² Score | MSE |
|-------|-----------|-----|
| Linear Regression (Scratch) | 0.808 | 0.00388 |
| Linear Regression (Using Sklearn) | 0.808 | 0.00388 |
| Decision Tree Regressor | 0.741 | 0.00422 |
| Random Forest Regressor | **0.856** | **0.00234** |

**Insight:**  
The **Random Forest Regressor** achieved the best performance, showing that **planned efficiency** and **performance bonus** were key drivers of productivity.

---

## ğŸ’¡ TASK 2: Retail Web Session Intelligence (RWSI)

### ğŸ“˜ Dataset
The **Retail Web Session Intelligence (RWSI)** dataset simulates **user browsing sessions** on a digital retail platform selling consumer and lifestyle products.  
Each record represents an **anonymized session**, capturing:
- **Behavioral Metrics:** `pageEngagementScore`, `visitDuration`, `clickRate`
- **Contextual Attributes:** `visitMonth`, `userCategory`, `region`, etc.
- **Target Variable:** `MonetaryConversion` (Yes/No)

### ğŸ¯ Objective
To develop a **classification model** that predicts the **likelihood of conversion** (purchase intent) based on user engagement, browsing behavior, and contextual factors.

### âš™ï¸ Methodology
- Outlier Detection via Scatter Plots & Range Inference  
- **Semi-Synthetic Balancing**: Created diverse â€œYesâ€ samples using feature-specific ranges + controlled randomness  
- One-Hot Encoding for Categorical Features  
- Feature Scaling and Normalization  
- Model Training using:
  - Logistic Regression (Standard & Scaled)
  - Decision Tree
  - Random Forest
  - XGBoost

### ğŸ“ˆ Results
| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|--------|-----------|------------|---------|-----------|----------|
| XGBoost | 0.865 | 0.87 | 0.86 | 0.86 | 0.94 |
| Random Forest | **0.867** | 0.87 | 0.87 | 0.87 | **0.94** |
| Decision Tree | 0.865 | 0.87 | 0.87 | 0.87 | 0.94 |
| Logistic Regression (Sklearn) | 0.815 | 0.82 | 0.82 | 0.81 | 0.91 |
| Logistic Regression (Scratch) | 0.810 | 0.81 | 0.81 | 0.81 | 0.90 |

**Insight:**  
Random Forest and XGBoost provided **balanced precision-recall tradeoffs** and **smooth convergence**, validating the effectiveness of the new **range-based oversampling** and **outlier refinement** strategies.

---

## ğŸ§  Key Learnings
- Importance of **domain-specific preprocessing** in both industrial and retail datasets.  
- Semi-synthetic balancing using **controlled randomness** preserved data authenticity.  
- Ensemble models like **Random Forest** generalized better under class imbalance.  
- Feature importance analysis linked **engagement metrics** (Task 2) and **planning metrics** (Task 1) to real-world outcomes.

---

## âš™ï¸ Environment & Tools
- **Platform:** Google Colab  
- **Language:** Python 3.12  
- **Libraries:** pandas, numpy, scikit-learn, xgboost, matplotlib, seaborn, plotly  
- **Visualization:** Correlation Maps, ROC Curves, Scatter Plots, Heatmaps  
- **Version Control:** GitHub

---

### ğŸ“ Reference
ğŸ“‚ Full reports and detailed documentation are available in the [`Documentation/`](https://github.com/Dipin-Raj/Classical_ML/tree/main/Report%20%26%20Documentation) folder.  
ğŸ§¾ All source notebooks are available in the [`Notebook/`](https://github.com/Dipin-Raj/Classical_ML/tree/main/Notebooks) directory.  
ğŸ“Š Visual results and graphs can be found in the [`Graphical Representations/`](https://github.com/Dipin-Raj/Classical_ML/tree/main/Graphs) section.

